# 在大型数据中心网络中数据包级检测

## Abstract

- 独特的问题
  * 规模
  * 通量(traffic volume)
  * 错误的多样性

- 问题解决
  * 辨识在大流量中被影响的包
  * 在多个网络设备中追踪这些包
  * 分析故障中的流量路径
  * 测试或是确认潜在的问题

Everflow 可以追踪特定的包, 通过实现了一个包过滤器, 他基于商用交换机的"匹配复制"机制.
他将捕获到的包重新分发到不同的分析服务器上(使用交换机内置的ASIC芯片的负载均衡), 他发送
指南探针去确认潜在的错误. 我们目前的实验演示了Everflow的扩展性, 并且分享了近6个月以来的
解决的问题, 这些问题来源于运行环境.


## Introduction

主动管理DCN的模型, 其中的设备可以观察, 分析, 并且及时纠正错误.

- 理解和调试在DCN中的错误是很有挑战的, 因为错误有各式各样:
  1. A=>B服务器中的一些包可能会有很高的延迟, 但是并不清楚是哪一个链接该负责
  2. 发往特定的一组服务器的数据包可能会被丢失, 即使丢包计时器没有表现异常
  3. 到虚拟IP的TCP连接可能会有断断续续的超时, 并且traceroute探测去调试时也会被负载均衡所阻塞
  4. 由于一组ECMP的连接而导致负载未均衡, 网络管理者并不知道问题是由流量大小的差异还是更深的原因

这些问题的诊断需要我们在包的粒度上进行测试. 

当我们遇到 
1. 不完善的接口
2. 软件切换错误
可能会导致影响一组特定包的看似随机的错误, 


- 建立可靠的包级别的探测系统有三个挑战
  1. 当前的SCN具有史无前例的网络流量
    将流量移动到商用的服务器中分析会导致网络拥塞, 甚至破坏网络.
  2. DCN的错误经常会发生在多跳或是多个交换机中, 并且有效的追踪需要智能的追溯网络中的一小
     部分包(不只是大海捞针, 还是要找到特定大小, 形状, 以颜色的针)
  3. 被动的追踪瞬间的网络快照, 将会被性能所限制.

Everflow: 一个网络检测系统 提供了高扩展性, 数据包层面的信息
与追踪单独的包不同的是, Everflow使用了"匹配复制"机制. 商用的交换机可以灵活的匹配
包的头部或是负载, 我们使用这一特性, 为我们的分析服务器镜像数据包

为交换机安装一组特定的匹配镜像规则, 我们可以既进行数据包级别的追踪, 并且将开销降低几个数量级

建造了一个可扩展的收集与分析流水线.

利用基于交换机的负载均衡来分配多个服务器在跟踪和理, 同时保持流级别的局部性以进行有效的分析.

Everflow允许探测包(特制的注入到网络中的数据包), 遵循预设的路径.
探测器可以验证单个设备的行为和性能


## Packet-level network telemetry

- 一些例子,(传统工具的局限以及数据包级别检测的优势)

  1. 静默的丢包. 交换机并不报告., 即使在发送端检测到了TCP报文的重发,
      也无法确定是那个交换机出现了问题, 通过连续的在交换机中检测特定数据包, 我们
      可以立刻发现最后一跳交换机, 以及被丢失的数据包期待的下一跳的交换机

  2. 静默的黑洞, 路由黑洞不在转发表中显示. 因此, 他不能被检查转发表的工具所检测到.
      静默的黑洞可能由于损坏的TCAM表条目而发生. 通静默的丢包一样, 我们也可以检测到
      静默的黑洞.

  3. 虚增的端到端延迟: 一个流量的高延迟, 很容易被终端主机所检测, 但很难利用传统工具进行调试
      通过包级别的追踪, 这个问题变得不重要, 我们可以获得每一跳的延迟

  4. 从错误的中间件路由中循环, 路由的问题由中间件产生而不是由交换机产生. 这可能是由于中间件
       错误的修改了包结构以及转发行为. 这样的问题不能通过检测交换机路由或是转发表来解决.
       给定一组数据包, 我们可以轻松的得知这样的问题, 因为该路径将会违反基本的原则例如无环

  5. 负载均衡 流量由一组ECMP链路不均匀的传递. 原始的通过ECMP的连接数进行的检测很有可能误报,
       因为链路的负载可能由不同大小的流量来决定. 即使 检测到负载不均衡, 计数器只能粗粒度
       (coarse-gained)的在调试中回答问题, 只能回答"这种不均衡是由符合特定模式的流量组成的
       吗?"这样的问题. 但是, 一个数据包级别的检测允许我们检测到映射在每条链路上的
       [5元组模型][5-tuple], 并且更加可靠.

  6. 协议Bug, 在实现BGP, PFC(优先级流量控制), RDMA(远程直接内存访问)的协议时可能会有bug.
       当协议行为异常时, 网络的性能和可性就受到了影响. 解决协议的问题是很有挑战的. 因为
       许多协议可能由第三方发供应商(交换机和网卡)并且也不容易被仪器探测.


## Overview of Everflow

3.1 介绍了在系统扩展性上的挑战, 并且介绍了解决挑战的关键点
- Design Chanllenges

  1. 可扩展的追踪与分析
  2. 被动追踪的限制

3.2 关键点

  1. 在交换机上的匹配及镜像
    1. 设计匹配规则去抓取每一个数据流
    2. 配置额外的匹配规则可以灵活的追踪. 精确的匹配是十分重要的, 我们很可能想要追踪来自特定
        程序的数据包, 一组特定的端口或是一组服务器. 需要在头部添加一位用以标记`debug`
    3. 我们的追踪不只覆盖数据包, 还有BGP, PFC, RDMA的流量包

  2. 可扩展的跟踪分析器
  3. 基于交换机的洗牌器 将商用交换机转变为一个硬件复用器
      在交换机中定义一个虚拟VIP, `VIP -> DIP`有映射表, 每个DIP将会对应一个分析服务器
      我们之后将会改变所有的交换器配置, 使他们向VIP发送数据包, 当数据包到达交换机时, 交换机
      会根据5元组重定向数据包, 到特定的`DIP`中.(可以通过增加相同VIP的复用器来提高转换能力??)
      *封装*后的包: 包封装经常会用在DCN的负载均衡以及网络虚拟化中, 
  4. 有针对性的探测: 注入任何数据包到任意的交换机中, 并且追踪数据包的流向.
      1. 一个有效应用就是根据采样或是聚合的数据恢复路径信息. 如果丢包错误是短暂的, 甚至我们
        都没有生成探针, 这样的错误是无法调试的, 但是我们可以接受.
      2. 生成我们想要的数据包, 并且让其游历我们期望的路径. 这样我们可以衡量往返的延迟.
    
## Trace Collection and analysis
  跟踪收集与分析流水线
  Controller, analyzer, storage, reshuffler

- 许多的应用使用了Everflow提供的包级别的信息
- Controller 去适配其他组件以及与应用程序交互
- 在初始化中, 配置交换机中的规则(符合规则的包将会被镜像至reshuffler中, 并导向analyzer中,
     结果将会存储在storage中)

4.1 Analyzer

- 每个analyzer保存了两个状态, packet trace 与 counter
  *packet trace* table of packet traces, 每个trace是一个链表, 保存了相同来源的数据包
  每个trace由5元组和原始包的IPID进行标记, 也包含了每一跳的信息, 包括交换机的IP地址, 时间戳,
  TTL, 源MAC地址, 和`DSCP/ECN`, 我们认为1s内没有新的数据包到达, 一条trace就算结束.
  对于每一条完整的`packet trace`, 分析器只确认两件事, `loop`以及`drop`. 一个`loop`表示
  相中的设备出现了多次. 一个`drop`表现为, 最后一跳的数据包与预期的最后一跳不相同. 这可以由
  `DCN topology`与`routing policy`进行计算.
  软件的负载均衡解决: 我们合并了原始的数据包`p_o`以及封装后的数据包`p_e`.
  每个analyzer只会向存储中写入异常出具, 调试数据, 或是对应的协议流量.
  剩下的大部分trace数据将会进行统计并放入计数器中. 最终`controller`会将analyzer中的数据
  聚合输出最终结果

  *Line load counters* 对于每一个连接来说, 分析器将会计算聚合负载. 除了这个, 他也会计算
    更细粒度(fine-grained)的负载. 比如拥有特定前缀的流量. 

  *Latency counters* 

  *Mirrored packet drop counters*

4.2 Controller APIs


## Everflow applications

- 延迟探查
- 包丢失调试器





[5-tuple]: https://azure.microsoft.com/en-us/blog/azure-load-balancer-new-distribution-mode/

