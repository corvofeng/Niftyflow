# 在大型数据中心网络中数据包级遥测

## Abstract

  在复杂网络中的错误调试经常需要在数据包级别的抓取和分析. 在这项任务中, 数据
中心由于规模, 流量以及错误的多样性而产生了独特的挑战. 为了能够及时的处理, DCN
的管理员必须 a) 辨识在大流量中被影响的包;  b) 在多个网络设备中追踪这些包; c) 
分析故障中的流量路径; d) 测试或是确认潜在的问题. 据我们所知, 目前没有工具可以
达到这项任务所需的特异性和规模.

  我们推出了Everflow, 一个针对数据中心网络的数据包级别的网络遥测系统. Everflow
可以追踪特定的包, 通过实现了一个包过滤器, 他基于商用交换机的"匹配复制"机制.
他将捕获到的包重新分发到不同的分析服务器上(使用交换机内置的ASIC芯片的负载均衡),
他发送指南探针去确认潜在的错误. 我们目前的实验演示了Everflow的扩展性, 并且分享
了近6个月以来的解决的问题, 这些问题来源于运行环境.

## Introduction

  从在线商业到智能设备, 数据中心网络是大型在线服务的核心. DCNs被大量的采用, 即使
是性能上的一点降低或是故障引起的超时都会造成数以百万计的损失. 以上这些要求我们
建立主动管理DCN的模型, 其中的设备可以观察, 分析, 并且及时纠正错误.

  理解和调试在DCN中的错误是很有挑战的, 因为错误有各式各样, 例如:
    1. A=>B服务器中的一些包可能会有很高的延迟, 但是并不清楚是哪一个链接该负责
    2. 发往特定的一组服务器的数据包可能会被丢失, 即使丢包计时器没有表现异常
    3. 到虚拟IP的TCP连接可能会有断断续续的超时, 并且traceroute探测去调试时也会被负载均衡所阻塞
    4. 由于一组ECMP的连接而导致负载未均衡, 网络管理者并不知道问题是由流量大小的差异还是更深的原因

  这些问题的诊断需要我们在包的粒度上进行测试. 由于不完善的接口或是交换机的软件故障
可能产生随机的错误, 并且影响一组具有特定特征的包, 这些特征可以是采取的路线,
包头部或是时间. 由于其微妙的关系, 对于这些错误, 通过分析网络流信息, 聚合统计
或是抽样的流量也很难调试. 我们转而追踪, 收集分析数据包级别的流量行为, 作为
数据包级别的遥测系统.

  建立可靠的包级别的探测系统有三个挑战, 首先, 当前的SCN具有史无前例的网络流量
一个庞大的DCN拥有超过10万个服务器, 每个都有10-40Gbps的网络流量. 在高利用率下
总流量很容易超过100Tbps. 对于今天的商用交换机来说, 分析其中的一部分流量都是棘手
的, 并且将流量移动到商用的服务器中分析会导致网络拥塞, 甚至破坏网络.

  其次, DCN的错误经常会发生在多跳或是多个交换机中, 并且有效的追踪需要智能的追溯
网络中的一小部分包, 以及基于复杂的匹配模式去查找, 例如协议头, 起始点, 甚至是
路径中的设备. 这个过程不只是大海捞针, 还是要找到特定大小, 形状, 以颜色的针.
目前存在的基于包级别的分析是不分青红皂白的追踪数据包, 对于大规模的DCN 或是复杂
的匹配模式, 他们也不能提供扩展性.

  而后, 被动的追踪瞬间的网络快照, 将会被性能所限制. 快照中的路径可能不足以判断
问题是暂时的或是永久的. 也不能提供足够的信息去定位错误. 例如, 我们看到一组包
在到达终点时停止, 我们无法判断出这是由于随机丢弃或是黑洞.

  我们推出了Everflow, 一个网络检测系统 提供了高扩展性以及弹性的访问数据包层面
的信息, 与追踪单独的包不同的是, Everflow使用了"匹配复制"机制. 商用的交换机可以
灵活的匹配包的头部或是负载, 我们使用这一特性, 为我们的分析服务器镜像数据包
为交换机安装一组精心选定的匹配镜像规则, 我们可以既进行数据包级别的追踪,
并且将开销降低几个数量级. 为了快速的在大量的数据包路径中搜索, 我们建造了一个
可扩展的收集与分析流水线. 我们使用基于交换机的负载均衡功能来分布多个服务器上的
跟踪和处理, 同时保持流级别的局部性以进行有效的分析. 最后, Everflow允许探测包 --
特制的注入到网络中的数据包, 遵循预设的路径. 探测器可以验证单个设备的行为和性能

  Everflow 已经在2014年8月在微软的DCN上部分部署了, 包括有37太交换机的小集群以及
拥有440台交换机的大集群中. 这些集群承载了巨大的应用流量. 我们也有选择的
在其他生产集群中部署Everflow来追踪网络错误. 我们抓取一些部署经历, 通过描述几个
有代表性的调试案例(在第7章), 包括延迟问题, 包丢失, 路由环路, ECMP负载不均衡,
以及RDMA协议上的特定问题. 在每个例子中, 我们描述了可观察到的症状, 所用的步骤,
以及如何使用Everflow得到结果.

  我们执行详细的微基准来量化Everflow在关键方面的表现, 包括数据包分析速率,
带宽存储开销, 以及整体系统的扩展性. 我们的评估一致表明, Everflow的所有组件
需要了较低的开销, 并且能够很好的适应大型的DCN和100Tbps的流量.


  在开发Everflow时,我们试图解决对可扩展的数据包级遥测的真正需求,这种遥测可
以调试难以通过传统技术解决的故障. 我们优先于功能或功能的可用性和简单性. Everflow
工作在商用交换机上而且不需要其他特殊的硬件, 我们在Everflow上的工作表明他对
大型DCN的支持, 并且为网络管理员提供了巨大的好处.

## Packet-Lever network telemetry

  运营DCN包括各种各样的硬件和软件组件, 包括多种类型的交换机, 负载均衡器和专用
于处理和存储的服务器. 错误可能会由于单个或是几种组件所造成, 这使得调试变得十分
困难. 在这一章, 我们描述了在一些发生在我们大型DCN上的错误例子, 用以显示 传统
工具的局限以及数据包级别检测的优势.

  **静默的丢包**: 罪魁祸首的交换机并不报告(尽管计数器为0). 这种状况可能是由于
交换机上的软件bug或是硬件错误, 即使在发送端检测到了TCP报文的重发, 但也会因为交换机
较多也无法确定是那个交换机出现了问题. 通过连续的在交换机中检测特定数据包, 我们
可以立刻发现最后一跳交换机, 以及被丢失的数据包期待的下一跳的交换机. 然后我们发送
探针至下一跳的交换机中来确定罪魁祸首.

  **静默的黑洞**: 路由黑洞不在转发表中显示. 因此, 他不能被检查转发表的工具所检测到.
静默的黑洞可能由于损坏的TCAM表条目而发生. 同静默的丢包一样, 我们也可以检测和定位到
静默的黑洞. 

  **虚增的端到端延迟**: 一个流量的高延迟, 很容易被终端主机所检测, 但很难利用传统
工具进行调试. 但是通过包级别的追踪, 这个问题变得不重要, 我们可以获得每一跳的延迟

  **从错误的中间件路由中循环**: 路由的问题由中间件产生而不是由交换机产生. 这可能
是由于中间件错误的修改了包结构以及转发行为. 这样的问题不能通过检测交换机路由或是
转发表来解决. 给定一组数据包, 我们可以轻松的得知这样的问题, 因为该路径将会违反
基本的原则例如无环

  **负载均衡**: 流量由一组ECMP链路不均匀的传递. 原始的通过ECMP的连接数进行的
检测很有可能误报, 因为链路的负载可能由不同大小的流量来决定. 即使检测到负载不均衡,
计数器只能粗粒度(coarse-gained)的在调试中回答问题, 只能回答"这种不均衡是由符合
特定模式的流量组成的吗?"这样的问题. 但是, 一个数据包级别的检测允许我们检测到映射
在每条链路上的[5元组模型][5-tuple], 并且更加可靠.

  **协议Bug**: 在实现BGP, PFC(优先级流量控制), RDMA(远程直接内存访问)的协议时
可能会有bug. 当协议行为异常时, 网络的性能和可性就受到了影响. 解决协议的问题是很有
挑战的. 因为许多协议可能由第三方供应商(交换机和网卡)实现, 并且也不容易被仪器探测.
追踪这些协议的数据包提供了一种可靠的方案来检测协议缺陷. 这里, 网络遥测系统是十分
适合的, 因为基于主机的追踪系统昂贵(对于RDMA), 不可达(PFC和BGP).

## Overview of everflow

  这一章介绍了在系统扩展性上的挑战, 并且介绍了解决挑战的关键点.

###  Design challenges

**可扩展的分析与追踪**. 面向大型DCN的遥测系统, 第一个挑战就是路径追踪的扩展性.
就像先前提到的一样, 大型DCN(10W以上的服务器)中流量轻松就能达到100Tbps, 追踪这样
规模的数据包需要大量的网络资源与服务器资源, 想象一种典型的情况, 平均每个包的大小为
1000bytes, 每个镜像数据包64bytes(只包括头部信息) 并且网络的直径为5跳. 如果我们
简单的追踪每个数据包的每一跳作为数据包历史, 数据流量就会有这么多
$\frac{64B}{1000B} \times 5(hops) \times 1000(Tbps) = 32(Tbps)$, 如此高的追踪流量
将会造成网络拥塞和丢包, 特别是当网络利用率很高时.

  另一个扩展性的挑战是路径分析. 因为商用交换机有限的存储与CPU运算能力, 追踪数据包
必须由服务器来完成. 即使假设一个服务可以处理10Gbps的流量, 我们需要
$\frac{32Tbps}{10Tbps} = 3200$这么多服务器来处理, 这也是很大的开销.

  天真的做起来, 情况会更糟. 对于大部分的分期, 同一路径每一跳的数据包应该被发送到
相同的分析服务器中. 这个要求可以通过向一些洗牌器发送数据包来解决, 而后根据数据包
的头部hash值将数据包改组, 增加洗牌的服务器将会需要增加一倍的服务器.

**被动追踪的限制**. 在现实中, 被动的网络追踪可能不能提供解决问题的有效信息. 图一
显示了一种情况. TODO: 图介绍


### 3.2 Key ideas

 Everflow解决上述问题有四个关键点, 前三个是为了解决扩展性, 最后一个是未来解决
被动追踪的限制.

**在交换机上的匹配及镜像**. 商用交换机可以基于事先订立的规则来匹配, 然后执行某些
操作(镜像和封装), 而且这并不影响原有数据包的转发行为.Everflow使用这一点来减少
追踪开销. 特别的是, 我们设计了三种类型的匹配规则去解决DCN的一般错误. 这个规则绝不
是详尽的, 可以通过扩展规则来应对其他类型的错误.

  首先, 我们设计匹配规则去抓取每一个数据流, 最直接的方法是随机的抓取n个包中的一个,
这是严重偏向大流量的. 在DCN中, 数据流的大小分布是十分不均的, 这样就会错过了许多
小流量. 而那些小流量通常是面向用户的, 交互服务是有严格的性能要求的. 为了将小流量
覆盖到, 我们建立了一条新的规则, 这套新的规则基于TCP的SYN, FIN, RST数据包区域,
因为DCN的流量大部分由TCP表示, 这个规则允许我们追踪DCN中的每个TCP流量.

  而后, 我们配置额外的匹配规则可以灵活的追踪. 基础的TCP匹配不能抓取每一种类型的
错误, 例如, TCP流中部的数据包丢失. 例如, 我们可能想要追踪特定程序的数据包, 特定
的端口, 或是在某些服务器对之间. 为了允许可扩展的追踪, 我们允许数据包头部有一位被
标记为debug. 这个标记标准可以以任何方式定义, 只要全部的追踪影响低于一个阈值. 在
交换机中, 我们安装了一条规则去追踪任何有debug位的数据包, 就像软件开发者在编译期间
开启debug标志来启用细粒度的追踪, 因此, Everflow可以有效追踪常规数据包的任何子集.

  最后, 我们的追踪不只覆盖数据包. 一个DCN中有一小部分流量与网络协议相关, 例如
BGP, PFC, RDMA的流量包. 我们将其称为协议流量用以区分正常的数据流量. 尽管纯粹的
协议流量是很小的, 但却与DCN的健康相关. 因此, Everflow的规则中追踪所有协议流量.

**可扩展的跟踪分析器**. 尽管匹配规则限制了一些追踪开销, 但是因为DCN的规模庞大,
全部的追踪流量还是可能会很大. 为了减少这样的分析开销, 我们观察到, 在任何时候, 只有
一小部分被追踪的数据包(<0.01%)会出现异常行为(例如循环或丢弃). 这一现象的驱动我们
将正常包与异常包区别来对待, 例如, 前者保留详细记录每一跳的状态, 而后者的每个设备
状态, 我们在4.1章描述, 这样不同的对待使得我们减少了分析开销(三个数量级). 如果需要,
我们可以有选择的恢复丢失的信息, 只要通过探针追踪即可(如下所诉).

**基于交换机的洗牌器**. 就像在3.1中描述的一样, 我们需要一个开销很低的方式去将追踪
流量改组. 我们使用了先前所做的工作, 将商用交换机转变为一个硬件复用器来负载均衡.
首先在复用器中中定义一个虚拟VIP,  他将映射到一组`DIP`, 每个DIP将会对应一个分析
服务器. 我们之后将会改变所有的交换器配置, 使他们向VIP发送数据包, 当数据包`p`到达
复用器时, 复用器会根据5元组重定向数据包, 到特定的`DIP`中. 这就保证了具有相同
五元组的追踪数据包将会重定向到相同的DIP中.

  一个复用器可以利用交换机的能力来重定向流量(> 1Tbps). 这要比普通10Gbps的服务器
网卡快至少100倍, 并且极大的减少了重定向的开销, 而且可以通过增加相同VIP的复用器
来提高转换能力.

  我们需要注意封装后的数据包. (目前为止, 我们先忽略掉追踪数据包也被分装这一现象,
这个将会在6.1章讨论). 包封装经常会用在DCN的负载均衡以及网络虚拟化中. 图三TODO: 图问题
举了一个例子, 软件负载均衡器如可以打破追踪分析. 原始的数据包p的目的IP是 `VIP`(
复用器的IP), 一个复用器可以使用新的`DIP`(分析服务器)作为数据包p的新的目的IP.
这样做, 原始的数据包p和封装后的数据包p将会被发送至不同的分析器中, 也会被单独的处理.
为了避免这个错误, 我们安装了一个规则去匹配封装数据包的内层头部, 并且也配置复用器
对内层头部进行hash操作. 这允许我们追中数据包p的完整路径.

**有针对性的探测**. 像先前提出的那样, 丢包可能是由于多种原因发生的. 有时, 只进行
被动的追踪可能不会有效区分不同的可能性. 这样的歧义产生了如下的问题: 如果我们可以
任意的重放数据包路径呢? 更直接的是, 如果我们可以注入任何想要构造的数据包到任意的
交换机中, 并且追踪被注入的数据包(设置debug位)的行为, 我们称之位探针, 将会在6.2
章进行详细套路.

  一个有效应用就是根据采样或是聚合的数据恢复路径信息. 为了恢复任何数据包p的路径,
我们只需要将数据包p注入到第一跳的交换机中, 当然设置了debug位.

  进一步来讲, 探针克服了被动追踪时的限制. 在图1的例子中TODO: 图, 我们可以注入
许多数据包p的被封到交换机S2中, 去探测丢包是否是永久的. 另外, 我们也可以使用不同
的5元组模式来构造探针数据包, 以此来看看丢包是否是针对特定的5元组. 这样的探针不能
调试临时错误, 因为他们可能在探针初始化前就消失了. 我们认为这样是可以接受的, 因为
永久的错误要比临时错误更有影响.

  我们也扩展了探针, 使其不止能注入数据包至合适的交换机, 也能创造数据包并让其游历
我们期望的路径. 这个扩展使得我们可以衡量网络中任意链接的延迟. 就像在图2TODO: 图
一个探针p可被要求遍历S1->S2->S1, 因为p穿过了$S_{1}$两次, $S_{1}$就能产生两个
数据包p的时间$t_{1}$和$t_{2}$, 并且$t_{2} - t_{1}$就等于$S_{1}, $S_{2}$的
往返延迟.

  因为目前的许多交换机不提供时间戳函数, 我们不能直接的获取$t_{1}$, $t_{2}$.
然而, 两个数据包p在时间上是接近的, 并且也要经过相同的路径达到分析器. 因此, 我们
计算他们到达分析器的不同的时间之差约等于$t_{2} - t_{1}$.

## Trace collection and analysis

  现在我们退出了路径收集与分析的流水线, 就像在图4 TODO:图中显示的一样, 他由四个
主要组件组成: 控制器, 分析器, 存储器以及洗牌器. 在顶层, 有许多应用程序使用由
Everflow提供的信息去调试网络错误. 控制器负责协调其他组件以及和应用程序交流. 在
初始化过程中, 它配置交换机上的规则. 被匹配到的数据包将会镜像至洗牌器中, 而后会直接
到达分析器中, 而后将分析结果传递至存储器中. 控制器也提供了一些API, 允许Everflow
的应用程序去查询分析结果, 并且在主机上设置debug位. 我们将在这一章讨论分析器和
控制器, 在第六章讨论洗牌器和存储器.


### 4.1 Analyzers

  分析器是一群分布式的服务器, 每个服务器处理一部分追踪流量. 洗牌器将会均衡分析器
上的负载, 并且保证具有相同5元组的数据包会发送至相同的分析器中. 每个分析器保存了
两种类型的状态, packet trace 与 counter.

  **packet trace**. 分析器保存了一组追踪项的表格, 每个追踪项是一个链表, 保存了
相同来源的数据包,  每个追踪项由5元组和原始包的IPID进行标记, 也包含了每一跳的信息,
包括交换机的IP地址, 时间戳, TTL, 源MAC地址, 和`DSCP/ECN`, 我们认为1s内没有新
的数据包到达分析器, 一条trace就算结束(这比端到端延迟小得多)

  对于每一条完整的`packet trace`, 分析器只确认两件事, `loop`以及`drop`. 
一个`loop`表示相中的设备出现了多次. 一个`drop`表现为, 最后一跳的数据包与预期的
最后一跳不相同. 这可以由`DCN topology`与`routing policy`进行计算. 例如,
发往DCN内部IP地址的数据包期望的最后一跳是连接到IP地址的ToR交换机, 发往外部IP地址
的数据包最后一跳是DCN的边界交换机.

  为了正确处理由于软件负载均衡造成的数据包分装. 如果$p_{e}$的内部IP头的5元组和
IPID与$p_{o}$的头部相同, 我们合并原始的数据包$p_{o}$以及封装后的数据包$p_{e}$.

  尽管使用了"匹配镜像"机制, 数据包的总量还是很大. 为了减少存储器的开销, 每个分析器
写入存储器的只有异常行为(循环或丢包), 有debug位的探针, 或是重要协议(PFC和BGP).
对于其他追踪(大量而且普通), 每个分析器会定时(10s)来向存储器写入各种类型的计数器中.
最终`controller`会将analyzer中的数据聚合输出最终结果

 **Line load counters**. 对于每一个连接来说, 分析器将会通过追踪路径来计算
聚合负载(数据包数量, 字节以及流). 除了这个, 他也会计算更细粒度(fine-grained)
的负载. 比如拥有特定前缀的流量或是DCN内部的流量. 这些细粒度的计数器可以动态的通过
Everflow的程序来添加或是删除.

  **Latency counters**. 分析器将会计算使用探针的每条链接的延迟, 对于每个经过
负载均衡器的数据包, 他也会计算负载均衡器的延迟. 这一过程在图3(b)有描述, 图中的
均衡器连接了Tor交换机, 并且镜像原始的数据包以及封装后的数据包从相同的交换机到达了
相同的分析器, 我们可以使用到达时间来估计复用器的延迟. 这个延迟中包括了复用器与
交换机的往返延迟, 但是与复用器的延迟相比是微不足道的. 为了节省空间, 分析器将单个
延迟样本量化为延迟直方图中预定义的区域.

  **Mirrored packet drop counters**. 一个镜像数据包也可能在达到分析器时被
丢弃, 我们是可以这样推测的. 图5显示了数据包p从交换机$S_{1}$到交换机$S_{2}$.
然而, p的追踪结果只包括$S_{2}$, 缺少了$S_{1}$, 清楚的表明了从$S_{1}$到分析器
的数据包丢失了. 在我们目前的部署中, 这样的丢包率是很低的(大概0.001%).

  有时, 镜像包的丢失有可能是某个洗牌器或是分析器的拥塞. 为了提高流水线的可靠性,
我们部署了多个洗牌器, 多个分析器在DCN的不同位置. 将镜像流量转移出来, 从那些展现出
高丢包率的洗牌器或是分析器中.

### 4.2 Controller APIs

  Everflow应用程序通过API与控制器进行交互, 进行错误调试. 使用这些API, 应用程序
可以查找数据包路径, 安装细粒度的负载计数器, 触发探针, 以及在加入debug位后追踪路径.

  **GetTrace(Filter, Condition, StartTime, EndTime)**被用来获取在StartTime
和EndTime中的数据包路径. Filter参数指定了数据包类型, 就像在WireShark中使用的
筛选器一样. 它允许基于以太网络, IP, TCP或是UDP的头部来过滤数据包, 也允许检测
镜像数据包的外层头部(包括发送数据包的交换机IP). 例如Filter
`ip.proto==6 && ip.dst == 196.16.0.0/16 && switch == 10.10.0.10`
匹配所有的到`192.16.0.0/16`并且由交换机`10.10.0.10`所镜像的流量. Condition
参数指定了一些不能从数据包头部提取的属性. 例如, 它允许过滤一些是否包含环路或是丢包,
或是软件负载均衡器延迟过大的数据包.

  **GetCounter(Name, StartTime, Endtime)**被用来取回在StartTime到EndTime
中计数器的数值, 每个计数器被定义为一个名称, 就像"SwitchX_PortY_TCP".

  **AddCounter(Name, Filter) & RemoveCounter(Name)**被用来动态的添加或
删除细粒度的计数器, Filter就像上述的一样.

  **Probe(Format, Interval, Count)**被用来启动探针. 探针的结果可以稍后通过
`GetTract()`和`GetCounter()`获取到. Interval和Count参数制定了频率和探针发送
的总数. Format参数制定了探针的数据包, 包括3层和4层的数据包头部. 它与先前的Filter
参数类似, 只要稍做改变即可支持数据包封装. 例如
`ip.src == SIP1,SIP2 && ip.dst == DIP1,DIP2 && ip.proto == 6`定义了一个
由IP到IP的TCP数据包, 外层源IP是SIP1, 外层目的IP是DIP1, 内层源IP是SIP2, 内层
目的IP是DIP2.

  **EnableDbg(Servers, Filter) & DisableDbg(Servers, Filter)**被用来在
特定的服务器上标记或清除debug位. Filter参数与上面相同. 控制器只有在全部的数据流量
没有超过系统负载时才接受EnableDbg这样的请求.


## Everflow Applications

  使用Everflow的API来编写程序是直接的. 现在, 我们展示了几个示例程序, 它们帮助
我们解决了在第二章描述的问题.


 **延迟探查**.

- 包丢失调试器











